import json
import logging
import math
import os
import uuid
import xml.etree.ElementTree as ET
from datetime import datetime

import requests
from osgeo import gdal, ogr
from owslib import crs
from owslib.csw import CatalogueServiceWeb
from owslib.fes import PropertyIsEqualTo
from owslib.wms import WebMapService
from shapely import (bounds, box, from_geojson, geometrycollections,
                     get_geometry, intersects, multipolygons, to_geojson,
                     union_all)
from slugify import slugify

from inspire_gpkg_cache.gpkg import Gpkg

logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))
log = logging.getLogger("SpatialDataCache")

class SpatialDataCache():
    """This is a central class for caching spatial datasets from remote services.

    :param data_configuration: Configuration of datasets, with identifiers and dataset type definition
    :type data_configuration: dict
    :param area_of_interest_geojson: Polygon of the area of interest in geojson format
    :type area_of_interest_geojson:  str, geojson
    :param catalogue_uri: Base uri of the catalogues capabilities endpoint, without CSW-parameters
    :type catalogue_uri: str
    """

    def __init__(self, data_configuration:dict, area_of_interest_geojson:str, catalogue_uri:str):
        """Constructor method
        """
        self.data_configuration = data_configuration
        self.area_of_interest_geojson = area_of_interest_geojson
        self.catalogue_uri = catalogue_uri
        self.csw = CatalogueServiceWeb(self.catalogue_uri)
        self.supported_formats = ["GeoTIFF", "GML", "Database", "shapefile", ]
        self.max_pixels = 3000
        self.max_features = 5000
        self.output_filename = 'spatialcache'
        self.area_of_interest_filename = 'area_of_interest'

    def create_initial_mask(self):
        """Function to generate a raster mask for the area of interest. The raster mask
        can be imported to the geopackage file to initialize the metadata extension. This 
        extension is only generated by gdal when importing raster data. So importing the mask
        prepare the geopackage for managing standardized metadata for raster and vector layers.
        """
        data_file_name = self.area_of_interest_filename + ".geojson" 
        open_option = "w"
        area_of_interest_file = open(data_file_name, open_option)
        area_of_interest_file.write(self.area_of_interest_geojson)
        area_of_interest_file.close()
        polygon = from_geojson(self.area_of_interest_geojson)
        polygon_box = polygon.bounds
        ds = gdal.Rasterize(self.area_of_interest_filename + ".tif",
                    data_file_name,
                    creationOptions=["COMPRESS=DEFLATE"],
                    outputBounds=[polygon_box[0], polygon_box[1], polygon_box[2], polygon_box[3]],
                    xRes=0.00001,
                    yRes=0.00001,
                    noData=0,
                    burnValues=255,
                    targetAlignedPixels=True,
                    inverse=True,
                    outputType=gdal.GDT_Byte)
        ds = None

    def resolve_dataset_metadata(self, fileidentifier):
        """Method that load the dataset metadata from the configured CSW by using the GetRecordById
        operation. The invocation is done via owslib.

        :param fileidentifier: MD_Metadata fileidentifier of an ISO metadata record. Fileidentifiers are often uuids.
        They identify teh metadata record itself and not the described dataset! TODO: identify the dataset by the usage
        of spatial datataset identifier - they identify the dataset itself!

        :type fileidentifier: str
        :return: :class:`owslib.iso.MD_Metadata` if resolves, `False` otherwise 
        """
        self.csw.getrecordbyid(id=[ fileidentifier ], outputschema='http://www.isotc211.org/2005/gmd')
        if fileidentifier in self.csw.records:
            return self.csw.records[ fileidentifier ]
        else:
            return False

    def extract_info_from_dataset_metadata(self, metadata):
        """extract format, bbox, spatial_dataset_identifier, resolution"""
        metadata_info = {}
        metadata_info['fileidentifier'] = ''
        metadata_info['spatial_dataset_identifier'] = ''
        metadata_info['spatial_res_type'] = ''
        metadata_info['spatial_res_value'] = ''
        metadata_info['spatial_res_value_uom'] = ''
        metadata_info['format'] = ''
        metadata_info['minx'] = ''
        metadata_info['miny'] = ''
        metadata_info['maxx'] = ''
        metadata_info['maxy'] = ''
        metadata_info['title'] = ''
        metadata_info['epsg_id'] = ''
        if metadata.identifier:
            metadata_info['fileidentifier'] = metadata.identifier
        if metadata.identification.bbox:
            metadata_info['minx'] = float(metadata.identification.bbox.minx)
            metadata_info['miny'] = float(metadata.identification.bbox.miny)
            metadata_info['maxx'] = float(metadata.identification.bbox.maxx)
            metadata_info['maxy'] = float(metadata.identification.bbox.maxy)
            log.info("bbox: " + str(metadata_info['minx']) + "," + str(metadata_info['miny']) + "," + str(metadata_info['maxx']) + "," + str(metadata_info['maxy']))
        if metadata.identification.uricode:
            metadata_info['spatial_dataset_identifier'] += metadata.identification.uricode[0]
        else:
            log.info("uricode not set!")
        if metadata.identification.uricodespace:
            metadata_info['spatial_dataset_identifier'] += metadata.identification.uricodespace[0]
        else:
            log.info("uricodespace not set!")
        log.info("Spatial Dataset Identifier from metadata record: " + metadata_info['spatial_dataset_identifier'])
        if metadata.identification.distance:
            if metadata.identification.uom[0] and metadata.identification.distance[0]:
                uom = metadata.identification.uom[0].split("#")[1]
                ground_resolution = float(metadata.identification.distance[0])
                metadata_info['spatial_res_type'] = "groundResolution"
                metadata_info['spatial_res_value'] = ground_resolution
                metadata_info['spatial_res_value_uom'] = str(uom)
                log.info("ground resolution: " + str(metadata_info['spatial_res_value']) + " " + str(uom))
            else:
                metadata_info['spatial_res_type'] = "scaleDenominator"
                metadata_info['spatial_res_value'] = float(metadata.identification.distance[0])
                log.info("scale denominator: " + str(metadata_info['spatial_res_value']))
        metadata_info['format'] = str(metadata.distribution.format)
        log.info("format: " + str(metadata_info['format']))
        if metadata.identification.title:
            metadata_info['title'] = metadata.identification.title
        if str(metadata.distribution.format) in self.supported_formats:
            log.info("dataset metadata format supported!")
        if metadata.referencesystem.code:
            log.info(str(metadata.referencesystem.code))
            try:
                metadata_info['epsg_id'] = crs.Crs(metadata.referencesystem.code).code
            except:
                log.info("could not extract epsg id from metadata")
                metadata_info['epsg_id'] = 0
        return metadata_info
    
    def get_coupled_services(self, spatial_dataset_identifier):
        service_query = PropertyIsEqualTo('csw:OperatesOn', spatial_dataset_identifier)
        # look for srv:serviceTypeVersion to find the atom feeds
        self.csw.getrecords2(constraints=[service_query], maxrecords=20, outputschema='http://www.isotc211.org/2005/gmd')
        # debug list services
        for rec in self.csw.records:
            # log.info(self.csw.records[rec].identification.title)
            # log.info(self.csw.records[rec].identification.abstract)
            # log.info(self.csw.records[rec].serviceidentification.type)
            # log.info(self.csw.records[rec].serviceidentification.version)
            pass
        return self.csw.records

    def check_atom(self, service, dataset_type, spatial_dataset_identifier:str, epsg_id):
        if service.serviceidentification.type == 'download' and service.serviceidentification.version == 'predefined ATOM':
            log.info("found predefined atom")
            r = requests.get(service.distribution.online[0].url)
            # parse inspire service feed
            tree = ET.fromstring(r.text)
            # extract code and codespace from spatial_dataset_identifier - should be separated by / as demanded from inpsire
            code = spatial_dataset_identifier.split('/')[-1]
            # codespace is everything before this
            codespace = spatial_dataset_identifier.rstrip(code)
            # extract link for entry of the needed dataset identifier
            entry = tree.findall("./{http://www.w3.org/2005/Atom}entry/{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}spatial_dataset_identifier_code[.='" + code + "']/../{http://www.w3.org/2005/Atom}link[@rel='alternate']")
            if (len(entry) > 0):
                log.info(entry[0].attrib['href'])
                # get dataset feed
                r = requests.get(entry[0].attrib['href'])
                # log.info(r.text)
                tree = ET.fromstring(r.text)
                if dataset_type == 'raster':
                    format_mimetype = "image/tiff"
                if dataset_type == 'vector':
                    format_mimetype = "application/json; subtype=geojson"
                # find entry with format and crs from metadata (original crs)
                # http://www.opengis.net/def/crs/EPSG/25832 or ...
                entry = tree.findall("./{http://www.w3.org/2005/Atom}entry/{http://www.w3.org/2005/Atom}category[@term='http://www.opengis.net/def/crs/EPSG/" + str(epsg_id) + "']/../{http://www.w3.org/2005/Atom}link[@type='" +  format_mimetype + "']")
                if len(entry) > 0:
                    # log.info('found appropriate atom dataset feed with crs from metadata and supported mimetype!')
                    return service, False
        return False, False

    def check_wms(self, service, spatial_dataset_identifier:str):
        if service.serviceidentification.type == 'view' and service.serviceidentification.version == 'OGC:WMS 1.1.1':
            # check ouput format
            for online_resource in service.distribution.online:
                # log.info(str(online_resource.name) + " : " + str(online_resource.function) + " : " + str(online_resource.url))
                if online_resource.url:
                    r = requests.get(online_resource.url)
                    tree = ET.fromstring(r.text)
                    # check for tiff format
                    formats = tree.findall(".//GetMap/Format")
                    supports_tif = False
                    for format in formats:
                        # log.info(str(format.text))
                        if format.text == 'image/tiff':
                            supports_tif = True
                        if supports_tif:
                            log.info("WMS with supported output_format tif found: " + str(service.serviceidentification.title))
                            # check for dataset layer coupling
                            data_layer = tree.findall(".//Layer[Identifier='" + spatial_dataset_identifier + "']")
                            if len(data_layer) == 1:
                                log.info("WMS has a corresponding Layer with name: " + str(data_layer[0].find('Name').text))
                                return service, str(data_layer[0].find('Name').text)
        return False, False

    def check_oaf(self, service, spatial_dataset_identifier:str):
        if service.serviceidentification.type == 'download' and service.serviceidentification.version == 'ogcapifeatures':
            return service, False
        return False, False
    
    def check_wfs(self, service, spatial_dataset_identifier:str):
        if service.serviceidentification.type == 'download' and service.serviceidentification.version == '2.0.0':
            for online_resource in service.distribution.online:
                # check all uris for getcapabilities 
                log.info(str(online_resource.name) + " : " + str(online_resource.function) + " : " + str(online_resource.url))
                if online_resource.url:
                    r = requests.get(online_resource.url)
                    tree = ET.fromstring(r.text)
                    # check for tiff format
                    formats = tree.findall(".//{http://www.opengis.net/ows/1.1}OperationsMetadata/{http://www.opengis.net/ows/1.1}Operation[@name='GetFeature']/{http://www.opengis.net/ows/1.1}Parameter[@name='outputFormat']/{http://www.opengis.net/ows/1.1}AllowedValues/{http://www.opengis.net/ows/1.1}Value")
                    supports_geojson = False
                    for format in formats:
                        log.info(str(format.text))
                        if format.text == 'application/json; subtype=geojson':
                            supports_geojson = True
                    if supports_geojson:
                        log.info("geojson supported")
                    else:
                        log.info("server does not support geojson! - exit")
                    # check for inspire spatial dataset identifier
                    spatial_dataset_identifier_codes = tree.findall(".//{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}ExtendedCapabilities/{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}SpatialDataSetIdentifier/{http://inspire.ec.europa.eu/schemas/common/1.0}Code")
                    spatial_dataset_identifier_codespaces = tree.findall(".//{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}ExtendedCapabilities/{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}SpatialDataSetIdentifier/{http://inspire.ec.europa.eu/schemas/common/1.0}Namespace")
                    spatial_dataset_identifier_array = []
                    for i in range(len(spatial_dataset_identifier_codes)):
                        spatial_dataset_identifier_array.append(str(spatial_dataset_identifier_codespaces[i].text) + str(spatial_dataset_identifier_codes[i].text))
                        log.info(str(spatial_dataset_identifier_codespaces[i].text) + str(spatial_dataset_identifier_codes[i].text))
                    # find 
                    index_of_featuretype = spatial_dataset_identifier_array.index(spatial_dataset_identifier)
                    log.info("index of featuretype: " + str(index_of_featuretype))
                    if isinstance(index_of_featuretype, int):
                        # get name of featuretype
                        featuretypenames = tree.findall(".//{http://www.opengis.net/wfs/2.0}FeatureTypeList/{http://www.opengis.net/wfs/2.0}FeatureType/{http://www.opengis.net/wfs/2.0}Name")
                        for featuretype in featuretypenames:
                            log.info("Found featuretype: " + featuretype.text)
                        if len(featuretypenames) == 1:
                            featuretype_to_request = featuretypenames[0].text
                            log.info("wfs with featuretype " + featuretype_to_request + " found for downloading data!")
                        else:
                            log.info("no featuretype found for downloading dataset!")
        return False, False

    def check_download_options(self, service, spatial_dataset_identifier:str):
        """
        Method for check which possible options for downloading the dataset exists  
        """
        possible_dataset_type = None
        service_type = None
        service_version = None
        service_resource_name = None
        access_uri = None
        error_messages = []
        # classify type of service (wms/wfs/oaf/atom/...) from the information in the service metadata
        if service.serviceidentification.type in ['view']:
            if service.serviceidentification.version in ['OGC:WMS 1.1.1', 'OGC:WMS 1.3.0']:
                service_type = 'wms'
                # check uri for getcapabilities
                # first check list of download urls from distribution metadata
                for online_resource in service.distribution.online:
                    if online_resource.url:
                        # TODO: sanitize access url - add params if not already given 
                        access_uri = online_resource.url
                        r = requests.get(online_resource.url)
                        # TODO: check for http 401
                        tree = ET.fromstring(r.text)
                        if tree.tag in ['WMS_Capabilities', 'WMT_MS_Capabilities']:
                            # some wms found
                            service_type = 'view'
                            service_version = 'OGC:WMS ' + str(tree.attrib['version'])
                            # check for tiff format
                            formats = tree.findall(".//GetMap/Format")
                            supports_tif = False
                            for format in formats:
                                if format.text == 'image/tiff':
                                    supports_tif = True
                            if supports_tif:
                                data_layer = tree.findall(".//Layer[Identifier='" + spatial_dataset_identifier + "" + "']")
                                if len(data_layer) == 1:
                                    service_resource_name = str(data_layer[0].find('Name').text)
                                    possible_dataset_type = 'raster'
                                    break
                                else:
                                    error_messages.append('WMS ' + str(tree.attrib['version']) + ' : No corresponding layer found for the requested spatial dataset identifier')
                            else:
                                error_messages.append('WMS ' + str(tree.attrib['version']) + ' : outputFormat image/tiff not supported')
        if service.serviceidentification.type in ['download']:
            if service.serviceidentification.version in ['ogcapifeatures', 'OGC-API Features', '']:
                service_type = 'oaf'
                possible_dataset_type = 'vector'
            if service.serviceidentification.version in ['2.0.0', 'OGC:WFS 2.0.0', 'OGC:WFS 1.1.0', 'OGC:WFS 2.0']:
                service_type = 'wfs'
                # check uri for getcapabilities
                # check version
                # check getcapabilities - maybe some 401 - store needed auth type
                # check version
                # check if featuretype for dataset is available
                # check formats and crs
                possible_dataset_type = 'vector'
            if service.serviceidentification.version == 'predefined ATOM':
                service_type = 'atom'
                # check returnable formats
                # need to read all hrefs from dataset feed!
                # check uri for service feed
                # check if uri to dataset feed is available
                # parse datasetfeed
                # extract possible formats/mimetypes and crs
                possible_dataset_type = 'vector' # or possible_dataset_types.append('raster')
        if possible_dataset_type is None:
            error_messages.append('Service is not usable for downloading dataset') 
        return service_type, service_version, possible_dataset_type, access_uri, service_resource_name, error_messages

    def get_appropriate_service(self, dataset_type:str, spatial_dataset_identifier:str, services, epsg_id):
        if dataset_type == 'raster':
            log.info("check services for raster download")
            for service in services:
                # option 1: wms with output_format image/tif
                found_service, found_layer = self.check_wms(services[service], spatial_dataset_identifier)
                if found_service:
                    return found_service, found_layer, 'raster_wms'
                # option 2: atom feeds with mimetype image/tif
                found_service, found_layer = self.check_atom(services[service], dataset_type, spatial_dataset_identifier, epsg_id)
                if found_service:
                    return found_service, found_layer, 'raster_atom'
            return False, False, False
        if dataset_type == 'vector':
            log.info("check services for vector download")
            # TODO: test list of services in special order - wfs, ogcapifeatures, atom feed, link, ...
            for service in services:
                # option 1: wfs with output_format geojson
                found_service, found_layer = self.check_wfs(services[service], spatial_dataset_identifier)
                if found_service:
                    return found_service, found_layer, 'vector_wfs'
                # option 2: oaf
                found_service, found_layer = self.check_oaf(services[service], spatial_dataset_identifier)
                if found_service:
                    return found_service, found_layer, 'vector_oaf'
                # option 3: atom feeds with mimetype application/geojson
                found_service, found_layer = self.check_atom(services[service], dataset_type, spatial_dataset_identifier, epsg_id)
                if found_service:
                    return found_service, found_layer , 'vector_atom'
            return False, False, False

    def download_datacache(self, metadata_info, download_service, resource_name, dataset_type:str, download_service_type:str):
        # log.info(type(download_service))
        dataset_file_array = []
        polygon_array = []
        polygon = from_geojson(self.area_of_interest_geojson)
        if download_service_type == 'raster_wms':
            log.info("download data via wms interface")
            bboxes = self.calculate_wms_bboxes(metadata_info)
            log.info("resource(layer/featuretype name): " + resource_name)
            # get first url - as demanded by INSPIRE
            wms_capabilities_url = download_service.distribution.online[0].url
            wms = WebMapService(wms_capabilities_url)
            inc = 0
            for geom_box in bboxes:
                # load image via owslib
                # TODO: if server does not support gtiff - download png, generate tfw and convert to gtiff!
                
                tmp_image = wms.getmap(layers=[resource_name], styles=['default'], srs='EPSG:4326', bbox=(geom_box.bounds[0], geom_box.bounds[1], geom_box.bounds[2], geom_box.bounds[3]), size=(self.max_pixels, self.max_pixels), format='image/tiff', transparent=True)
                out = open(metadata_info['fileidentifier'] + "_wms_" + resource_name + "_" + str(inc) + ".tif", 'wb')
                out.write(tmp_image.read())
                out.close()
                dataset_file_array.append(metadata_info['fileidentifier'] + "_wms_" + resource_name + "_" + str(inc) + ".tif")
                log.info("image " + str(inc) + " saved!")
                inc = inc + 1
        if download_service_type == 'vector_oaf':
            ogc_api_features_base_url = download_service.distribution.online[0].url
            polygon_box = polygon.bounds
            # add bbox value, limit, and format
            # first get only 10 objects to check the number of objects in the bbox of the area of interest
            download_url = ogc_api_features_base_url + "?f=json&limit=10&bbox=" + str(polygon_box[0]) + "," + str(polygon_box[1]) + "," + str(polygon_box[2]) + "," + str(polygon_box[3])
            # log.info(download_url)
            r = requests.get(download_url)
            json_result = json.loads(r.text)
            if 'numberMatched' in json_result.keys():
                log.info('numberMatched: ' + str(json_result['numberMatched']))
            if 'numberReturned' in json_result.keys():
                log.info('numberReturned:' + str(json_result['numberReturned']))
            bboxes = []
            # if number of matched is greater then number returned - build bboxes
            if 'numberMatched' in json_result.keys() and 'numberReturned' in json_result.keys():
                if int(json_result['numberMatched']) > (json_result['numberReturned']):
                    bboxes = self.calculate_feature_bboxes(metadata_info, int(json_result['numberMatched']))
                else:
                    geom_box = box(polygon_box[0], polygon_box[1], polygon_box[2], polygon_box[3])
                    bboxes.append(geom_box)
            else:
                geom_box = box(polygon_box[0], polygon_box[1], polygon_box[2], polygon_box[3])
                bboxes.append(geom_box)
            inc_bboxes = 0 
            for bbox in bboxes:
                download_url = ogc_api_features_base_url + "?f=json&limit=100&bbox=" + str(bbox.bounds[0]) + "," + str(bbox.bounds[1]) + "," + str(bbox.bounds[2]) + "," + str(bbox.bounds[3])
                r = requests.get(download_url)
                out = open(metadata_info['fileidentifier'] + "_json _" + str(inc_bboxes) + ".geojson", 'w')
                out.write(r.text)
                out.close()
                dataset_file_array.append(metadata_info['fileidentifier'] + "_json _" + str(inc_bboxes) + ".geojson")
                inc_bboxes = inc_bboxes + 1
                log.info("json saved!")
        if download_service_type == 'raster_atom' or download_service_type == 'vector_atom':
            r = requests.get(download_service.distribution.online[0].url)
            # parse inspire service feed
            tree = ET.fromstring(r.text)
            # extract code and codespace from spatial_dataset_identifier - should be separated by / as demanded from inpsire
            code = metadata_info['spatial_dataset_identifier'].split('/')[-1]
            # codespace is everything before this
            codespace = metadata_info['spatial_dataset_identifier'].rstrip(code)
            log.info("predefined atom codespace: " + codespace)
            log.info("predefined atom code: " + code)
            # extract link for entry of the needed dataset identifier
            entry = tree.findall("./{http://www.w3.org/2005/Atom}entry/{http://inspire.ec.europa.eu/schemas/inspire_dls/1.0}spatial_dataset_identifier_code[.='" + code + "']/../{http://www.w3.org/2005/Atom}link[@rel='alternate']")
            if (len(entry) > 0):
                # get dataset feed
                r = requests.get(entry[0].attrib['href'])
                # log.info(r.text)
                tree = ET.fromstring(r.text)
                if dataset_type == 'vector':
                    format_mimetype = "application/json; subtype=geojson"
                    file_suffix = 'geojson'
                if dataset_type =='raster':
                    format_mimetype = "image/tiff"
                    file_suffix = 'tif'
                # find entry with format and crs from metadata (original crs)
                # http://www.opengis.net/def/crs/EPSG/25832 or ...
                entry = tree.findall("./{http://www.w3.org/2005/Atom}entry/{http://www.w3.org/2005/Atom}category[@term='http://www.opengis.net/def/crs/EPSG/" + str(metadata_info['epsg_id']) + "']/../{http://www.w3.org/2005/Atom}link[@type='" +  format_mimetype + "']")
                if len(entry) > 0:
                    inc_bboxes = 0 
                    for link in entry:
                        bbox_array = link.attrib['bbox'].split()
                        bbox_geom = box(float(bbox_array[1]), float(bbox_array[0]), float(bbox_array[3]), float(bbox_array[2]))
                        if intersects(polygon, bbox_geom):
                            polygon_array.append(bbox_geom)
                            log.info('intersects: ')
                            log.info(link.attrib['href'])
                            # download geojson data
                            r = requests.get(link.attrib['href'])
                            # save result to file 
                            out = open(metadata_info['fileidentifier'] + "_" + file_suffix + "_" + str(inc_bboxes) + "." + file_suffix, 'wb')
                            out.write(r.content)
                            out.close()
                            dataset_file_array.append(metadata_info['fileidentifier'] + "_" + file_suffix + "_" + str(inc_bboxes) + "." + file_suffix)
                            inc_bboxes = inc_bboxes + 1
                            log.info("tile saved!")
            else:
                log.info("don't found any entry for dataset in atom service feed!")

        return dataset_file_array
    
    def calculate_wms_bboxes(self, metadata_info):
        """
        returns an array of shapely box objects
        """
        polygon = from_geojson(self.area_of_interest_geojson)
        polygon_box = polygon.bounds
        # calculate delta lon in meter (lon is first value x)
        middle_phi = polygon_box[1] + (polygon_box[3] - polygon_box[1]) / 2
        delta_lon_deg = polygon_box[2] - polygon_box[0]
        delta_lat_deg = polygon_box[3] - polygon_box[1]
        delta_lon_m = 2 * math.pi * 6378137.0 * math.cos(360 / middle_phi * 2 * math.pi) / 360 * delta_lon_deg
        delta_lat_m = 2 * math.pi * 6378137.0 / 360 * delta_lat_deg
        if metadata_info['spatial_res_type'] == "groundResolution":
            log.info(metadata_info['spatial_res_value'])
            # calculate needed pixels in each direction
            pixel_lat = delta_lat_m / metadata_info['spatial_res_value']
            pixel_lon = delta_lon_m / metadata_info['spatial_res_value']
            incs_lat = math.ceil(pixel_lat / self.max_pixels) 
            incs_lon = math.ceil(pixel_lon / self.max_pixels)
            # width and height of boxes in deg
            window_lat = delta_lat_deg / (pixel_lat / self.max_pixels)
            window_lon = delta_lon_deg / (pixel_lon / self.max_pixels)
            bboxes = []
            # calculate lat/lon boxes width pixel width of 3000*3000
            for i in range(incs_lat):
                for j in range(incs_lon):
                    bbox_miny = polygon_box[0] + j * window_lon
                    bbox_minx = polygon_box[1] + i * window_lat
                    bbox_maxy = polygon_box[0] + (j + 1) * window_lon
                    bbox_maxx = polygon_box[1] + (i + 1) * window_lat
                    geom_box = box(bbox_miny, bbox_minx, bbox_maxy, bbox_maxx)
                    bboxes.append(geom_box)
            # for debugging purposes store geojson of boxes to file  
            geom_boxes_multipolygon = multipolygons(bboxes)
            geojson_boxes = to_geojson(geom_boxes_multipolygon)
            log.info(geojson_boxes)  
            # write to folder 
            """ 
            data_file_name = metadata_info['fileidentifier'] + "_wms_bboxes.geojson" 
            open_option = "w"
            bboxes_file = open(data_file_name, open_option)
            bboxes_file.write(geojson_boxes)
            bboxes_file.close()
            """
            return bboxes
        
    def calculate_feature_bboxes(self, metadata_info, number_of_features:int):
        """
        returns an array of shapely box objects
        """
        polygon = from_geojson(self.area_of_interest_geojson)
        polygon_box = polygon.bounds
        number_of_boxes = math.ceil(number_of_features / self.max_features)
        # calculate delta lon in meter (lon is first value x)
        #middle_phi = polygon_box[1] + (polygon_box[3] - polygon_box[1]) / 2
        delta_lon_deg = polygon_box[2] - polygon_box[0]
        delta_lat_deg = polygon_box[3] - polygon_box[1]
        #delta_lon_m = 2 * math.pi * 6378137.0 * math.cos(360 / middle_phi * 2 * math.pi) / 360 * delta_lon_deg
        #delta_lat_m = 2 * math.pi * 6378137.0 / 360 * delta_lat_deg
        width = math.sqrt((delta_lon_deg * delta_lat_deg) / number_of_boxes)
        n_rows = math.ceil(delta_lat_deg / width)
        n_columns = math.ceil(delta_lon_deg / width)
        bboxes = []
        for i in range(n_rows):
            for j in range(n_columns):
                bbox_miny = polygon_box[0] + j * width
                bbox_minx = polygon_box[1] + i * width
                bbox_maxy = polygon_box[0] + (j + 1) * width
                bbox_maxx = polygon_box[1] + (i + 1) * width
                geom_box = box(bbox_miny, bbox_minx, bbox_maxy, bbox_maxx)
                bboxes.append(geom_box)
        # for debugging purposes store geojson of boxes to file  
        geom_boxes_multipolygon = multipolygons(bboxes)
        geojson_boxes = to_geojson(geom_boxes_multipolygon)
        log.info(geojson_boxes)  
        # write to folder
        data_file_name = metadata_info['fileidentifier'] + "_features_bboxes.geojson" 
        open_option = "w"
        bboxes_file = open(data_file_name, open_option)
        bboxes_file.write(geojson_boxes)
        bboxes_file.close()
        return bboxes
    
    def clean_tmp_files(self, tmp_file_list):
        if tmp_file_list and len(tmp_file_list) > 0:
            for tmp_file in tmp_file_list:
                if os.path.isfile(tmp_file):
                    os.remove(tmp_file)
    
    def derive_metadata(self, orig_metadata_xml, type):
        # alter fileidentifier, metadata_date, bbox, format, crs, spatial_dataset_identifier (TODO - howto name it) of original metadata
        tree = ET.fromstring(orig_metadata_xml)
        # extract metadata date
        md_date = tree.findall("./{http://www.isotc211.org/2005/gmd}dateStamp/{http://www.isotc211.org/2005/gco}Date")
        # set new metadata date
        current_date = datetime.now()
        md_date[0].text = current_date.strftime('%Y-%m-%d')
        md_identifier = tree.findall("./{http://www.isotc211.org/2005/gmd}fileIdentifier/{http://www.isotc211.org/2005/gco}CharacterString")
        md_identifier[0].text = str(uuid.uuid4())
        crs_code = tree.findall("./{http://www.isotc211.org/2005/gmd}referenceSystemInfo/{http://www.isotc211.org/2005/gmd}MD_ReferenceSystem/{http://www.isotc211.org/2005/gmd}referenceSystemIdentifier/{http://www.isotc211.org/2005/gmd}RS_Identifier/{http://www.isotc211.org/2005/gmd}code/{http://www.isotc211.org/2005/gco}CharacterString")
        crs_code[0].text = "urn:ogc:def:crs:EPSG:4326"
        md_format = tree.findall("./{http://www.isotc211.org/2005/gmd}distributionInfo/{http://www.isotc211.org/2005/gmd}MD_Distribution/{http://www.isotc211.org/2005/gmd}distributionFormat/{http://www.isotc211.org/2005/gmd}MD_Format/{http://www.isotc211.org/2005/gmd}name/{http://www.isotc211.org/2005/gco}CharacterString")
        if type == 'vector':
            md_format[0].text = 'GeoJSON'
        if type == 'raster':
            md_format[0].text = 'GeoTIFF'
        # geographical extent
        polygon = from_geojson(self.area_of_interest_geojson)
        polygon_box = polygon.bounds
        west_bound_lon = tree.findall("./{http://www.isotc211.org/2005/gmd}identificationInfo/{http://www.isotc211.org/2005/gmd}MD_DataIdentification/{http://www.isotc211.org/2005/gmd}extent/{http://www.isotc211.org/2005/gmd}EX_Extent/{http://www.isotc211.org/2005/gmd}geographicElement/{http://www.isotc211.org/2005/gmd}EX_GeographicBoundingBox/{http://www.isotc211.org/2005/gmd}westBoundLongitude/{http://www.isotc211.org/2005/gco}Decimal")
        east_bound_lon = tree.findall("./{http://www.isotc211.org/2005/gmd}identificationInfo/{http://www.isotc211.org/2005/gmd}MD_DataIdentification/{http://www.isotc211.org/2005/gmd}extent/{http://www.isotc211.org/2005/gmd}EX_Extent/{http://www.isotc211.org/2005/gmd}geographicElement/{http://www.isotc211.org/2005/gmd}EX_GeographicBoundingBox/{http://www.isotc211.org/2005/gmd}eastBoundLongitude/{http://www.isotc211.org/2005/gco}Decimal")
        south_bound_lat = tree.findall("./{http://www.isotc211.org/2005/gmd}identificationInfo/{http://www.isotc211.org/2005/gmd}MD_DataIdentification/{http://www.isotc211.org/2005/gmd}extent/{http://www.isotc211.org/2005/gmd}EX_Extent/{http://www.isotc211.org/2005/gmd}geographicElement/{http://www.isotc211.org/2005/gmd}EX_GeographicBoundingBox/{http://www.isotc211.org/2005/gmd}southBoundLatitude/{http://www.isotc211.org/2005/gco}Decimal")
        north_bound_lat = tree.findall("./{http://www.isotc211.org/2005/gmd}identificationInfo/{http://www.isotc211.org/2005/gmd}MD_DataIdentification/{http://www.isotc211.org/2005/gmd}extent/{http://www.isotc211.org/2005/gmd}EX_Extent/{http://www.isotc211.org/2005/gmd}geographicElement/{http://www.isotc211.org/2005/gmd}EX_GeographicBoundingBox/{http://www.isotc211.org/2005/gmd}northBoundLatitude/{http://www.isotc211.org/2005/gco}Decimal")
        west_bound_lon[0].text = str(polygon_box[0])
        east_bound_lon[0].text = str(polygon_box[2])
        south_bound_lat[0].text = str(polygon_box[1])
        north_bound_lat[0].text = str(polygon_box[3])
        ET.register_namespace(prefix='gmd', uri='http://www.isotc211.org/2005/gmd')
        ET.register_namespace(prefix='gco', uri='http://www.isotc211.org/2005/gco')
        ET.register_namespace(prefix='gmx', uri='http://www.isotc211.org/2005/gmx')
        ET.register_namespace(prefix='xsi', uri='http://www.w3.org/2001/XMLSchema-instance')
        ET.register_namespace(prefix='gml', uri='http://www.opengis.net/gml/3.2')
        ET.register_namespace(prefix='xlink', uri='http://www.w3.org/1999/xlink')
        ET.register_namespace(prefix='xsi:schemaLocation', uri='http://www.isotc211.org/2005/gmd http://schemas.opengis.net/csw/2.0.2/profiles/apiso/1.0.0/apiso.xsd')
        return ET.tostring(tree)

    def check_options(self):
        for dataset in self.data_configuration['datasets']:
            log.info(dataset['file_identifier'])
            #self.resolve_dataset_metadata(dataset['file_identifier'])
            metadata = self.resolve_dataset_metadata(dataset['file_identifier'])
            if metadata:
                log.info("metadata found")
                metadata_info = self.extract_info_from_dataset_metadata(metadata)
                log.info("Try to download " + metadata_info['title'] + " - type: " + dataset['type'] + " - sdi: " + str(metadata_info['spatial_dataset_identifier']))
                services = self.get_coupled_services(str(metadata_info['spatial_dataset_identifier']))
                log.info("number of found services: " + str(len(services)))
                # debug - show service information
                for service in services:
                    log.info(services[service].serviceidentification.type + ' - ' + services[service].serviceidentification.version + ' : ' + services[service].distribution.online[0].url)
                    log.info(json.dumps(self.check_download_options(services[service], str(metadata_info['spatial_dataset_identifier']))))
                    # service_type, service_version, possible_dataset_type, access_uri, error_messages = self.check_download_options(services[service], str(metadata_info['spatial_dataset_identifier']))

    def generate_cache(self):
        """function to start generation of cache"""
        # delete geopackage if exists
        if os.path.isfile(self.output_filename + '.gpkg'):
            os.remove(self.output_filename + '.gpkg')
        # initialize new geopackage
        gpkg = Gpkg(self.output_filename + '.gpkg')
        # Initialize one raster object to build up metadata tables - they are only build when importing raster data!
        # https://towardsdatascience.com/use-python-to-convert-polygons-to-raster-with-gdal-rasterizelayer-b0de1ec3267
        # https://www.programcreek.com/python/example/101827/gdal.RasterizeLayer
        self.create_initial_mask()
        gpkg.add_tif_layer('area_of_interest_mask_tif', 'area_of_interest.tif')
        gpkg.add_geojson_layer( "area_of_interest_geojson", "area_of_interest.geojson")
        for dataset in self.data_configuration['datasets']:
            log.info(dataset['file_identifier'])
            #self.resolve_dataset_metadata(dataset['file_identifier'])
            metadata = self.resolve_dataset_metadata(dataset['file_identifier'])
            if metadata:
                log.info("metadata found")
                metadata_info = self.extract_info_from_dataset_metadata(metadata)
                log.info("Try to download " + metadata_info['title'] + " - type: " + dataset['type'] + " - sdi: " + str(metadata_info['spatial_dataset_identifier']))
                # delete old cache files
                if dataset['type'] == 'raster' and os.path.isfile(dataset['file_identifier'] + '.tif'):
                    os.remove(dataset['file_identifier'] + '.tif')
                if dataset['type'] == 'vector' and os.path.isfile(dataset['file_identifier'] + '.geojson'):
                    os.remove(dataset['file_identifier'] + '.geojson')
                services = self.get_coupled_services(str(metadata_info['spatial_dataset_identifier']))
                log.info("number of found services: " + str(len(services)))
                # debug - show service information
                for service in services:
                    log.info(services[service].serviceidentification.type + ' - ' + services[service].serviceidentification.version + ' : ' + services[service].distribution.online[0].url)

                download_service, resource_name, download_service_type = self.get_appropriate_service(dataset['type'], str(metadata_info['spatial_dataset_identifier']), services, metadata_info['epsg_id'])
                log.info("name of resource: " + str(resource_name))
                log.info(download_service)
                # get metadata string
                new_metadata_xml = self.derive_metadata(metadata.xml, dataset['type'])
                polygon = from_geojson(self.area_of_interest_geojson)
                polygon_box = polygon.bounds
                # TODO:  write area of interest to geopackage - as gtiff! - to generate the metadata structure
                if download_service:
                    dataset_file_array = self.download_datacache(metadata_info, download_service, resource_name, dataset['type'], download_service_type)
                    if len(dataset_file_array) > 0:
                        if dataset['type'] == 'raster':
                            dataset_aggregate_filename = dataset['file_identifier'] + '.tif'
                            dataset_bbox_filename = dataset['file_identifier'] + '_wms_bboxes.geojson'
                            # when datasets are downloaded via atom feeds, they are in the crs which was given in the dataset metadata
                            if download_service.serviceidentification.version == 'predefined ATOM':
                                gdal.Warp(dataset_aggregate_filename, dataset_file_array, format="GTiff", srcSRS="EPSG:" + str(metadata_info['epsg_id']), dstSRS="EPSG:4326",
                            options=["COMPRESS=LZW", "TILED=YES"])
                            else:
                                gdal.Warp(dataset_aggregate_filename, dataset_file_array, format="GTiff",
                            options=["COMPRESS=LZW", "TILED=YES"])
                            # add to geopackage
                            gpkg.add_tif_layer(slugify(metadata_info['title']) + '_' + dataset['file_identifier'] + '_tif', dataset['file_identifier'] + '.tif')
                            md_id = gpkg.add_table_metadata(slugify(metadata_info['title']) + '_' + dataset['file_identifier'] + '_tif', new_metadata_xml.decode("utf-8"))
                            tmp_files = dataset_file_array
                            tmp_files.append(dataset_aggregate_filename)
                            tmp_files.append(dataset_bbox_filename)
                            #self.clean_tmp_files(tmp_files)
                        if dataset['type'] == 'vector':
                            dataset_aggregate_filename = dataset['file_identifier'] + '.geojson'
                            dataset_bbox_filename = dataset['file_identifier'] + '_features_bboxes.geojson'
                            if download_service.serviceidentification.version == 'predefined ATOM':
                                epsg_id = metadata_info['epsg_id']
                            else:
                                epsg_id = 4326
                            for dataset_file in dataset_file_array:
                                #gdal.VectorTranslate(dataset['file_identifier'] + '.geojson', dataset_file, accessMode="append", srcSRS="EPSG:" + str(epsg_id), dstSRS="EPSG:4326")
                                gdal.VectorTranslate(dataset_aggregate_filename, dataset_file, accessMode="append", srcSRS="EPSG:" + str(epsg_id), dstSRS="EPSG:4326", spatFilter=polygon_box, spatSRS='EPSG:4326')
                            # add to geopackage
                            gpkg.add_geojson_layer(slugify(metadata_info['title']) + '_' + dataset['file_identifier'] + '_geojson', dataset['file_identifier'] + '.geojson')
                            md_id = gpkg.add_table_metadata(slugify(metadata_info['title']) + '_' + dataset['file_identifier'] + '_geojson', new_metadata_xml.decode("utf-8"))
                            tmp_files = dataset_file_array
                            tmp_files.append(dataset_aggregate_filename)
                            tmp_files.append(dataset_bbox_filename)
                        # delete downloaded files and aggregate file
                        log.info("clean tmp files: " + json.dumps(tmp_files))
                        self.clean_tmp_files(tmp_files)
                        # store original metadata as a parent metadata into geopackage
                        if md_id:
                            gpkg.add_original_metadata(md_id, metadata.xml.decode("utf-8"))
                        else:
                            log.info('metadata for geopackage layer has not been stored!')
                        # new_gpkg.add_geojson_layer( "area_of_interest_geojson", "area_of_interest.geojson")
                    else:
                        log.info("No download option for dataset " + metadata_info['title'] + " (" + metadata_info['spatial_dataset_identifier'] + ")" + " found in catalogue")
        # delete area_of_interest files
        self.clean_tmp_files([self.area_of_interest_filename + '.tif', self.area_of_interest_filename + '.geojson',])
        # show list of cached datasets
        #TODO: test for empty gpkg
        gpkg.get_content_list2()